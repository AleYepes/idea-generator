{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "file_paths = glob.glob('data/*.html')\n",
    "list_of_dfs = []\n",
    "for file_path in file_paths:\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    header = soup.find('div', class_='particle-table-header')\n",
    "\n",
    "    if not header:\n",
    "        print(f\"  --> Warning: Could not find a header in {file_path}. Skipping this file.\")\n",
    "        continue\n",
    "\n",
    "    columns = [cell.text.strip() for cell in header.find_all('aw-header-cell')]\n",
    "\n",
    "    rows = []\n",
    "    for row in soup.find_all('div', class_='particle-table-row'):\n",
    "        cells = row.find_all('ess-cell')\n",
    "        if len(cells) == len(columns):\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            rows.append(row_data)\n",
    "        else:\n",
    "            print(f\"  --> Warning: Mismatch between column count and cell count in a row within {file_path}.\")\n",
    "\n",
    "    df_current_file = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    date_str_match = re.search(r'keywords_(\\d{2}-\\d{2}-\\d{2})\\.html', file_path)\n",
    "    if date_str_match:\n",
    "        date_str = date_str_match.group(1)\n",
    "        year_part = date_str.split('-')[0]\n",
    "        full_year_str = '20' + year_part\n",
    "        corrected_date_str = f\"{full_year_str}-{date_str.split('-')[1]}-{date_str.split('-')[2]}\"\n",
    "        df_current_file['date'] = pd.to_datetime(corrected_date_str)\n",
    "    else:\n",
    "        df_current_file['date'] = pd.NaT\n",
    "\n",
    "\n",
    "    list_of_dfs.append(df_current_file)\n",
    "\n",
    "if list_of_dfs:\n",
    "    df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "else:\n",
    "    print(\"No valid HTML files were found or processed. The final DataFrame is empty.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "def to_snake_case(column_name):\n",
    "    s = re.sub(r'[\\s\\n]+', ' ', column_name).strip()\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[().]', '', s)\n",
    "    s = s.replace(' ', '_')\n",
    "    s = s.replace('top_of_page_', '')\n",
    "    return s\n",
    "\n",
    "original_columns = df.columns\n",
    "new_columns_map = {col: to_snake_case(col) for col in original_columns}\n",
    "df.rename(columns=new_columns_map, inplace=True)\n",
    "\n",
    "df = df.drop(columns=['account_status', 'ad_impression_share'])\n",
    "\n",
    "df['competition'] = df['competition'].map({'Low': 1, 'Medium': 2, 'High': 3})\n",
    "df['keyword'] = df['keyword'].apply(lambda x: re.sub(r'[\\s\\n]+', ' ', x).strip())\n",
    "\n",
    "df['avg_monthly_searches'] = df['avg_monthly_searches'].str.split(' –').str[0]\n",
    "df['avg_monthly_searches'] = df['avg_monthly_searches'].str.replace('K', '000', regex=False)\n",
    "df['avg_monthly_searches'] = df['avg_monthly_searches'].str.replace('M', '000000', regex=False)\n",
    "df['avg_monthly_searches'] = pd.to_numeric(df['avg_monthly_searches'], errors='coerce')\n",
    "\n",
    "for col in ['bid_low_range', 'bid_high_range']:\n",
    "    df[col] = df[col].str.replace('€', '', regex=False)\n",
    "    df[col] = df[col].str.replace('—', '', regex=False).str.strip()\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df['avg_bid'] = df[['bid_high_range', 'bid_low_range']].mean(axis=1)\n",
    "\n",
    "for col in ['three_month_change', 'yoy_change']:\n",
    "    df[col] = df[col].str.replace('%', '', regex=False)\n",
    "    df[col] = df[col].str.replace(',', '', regex=False)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df[col] /= 100\n",
    "\n",
    "df = (df.sort_values(by='date')\n",
    "        .drop_duplicates(subset='keyword', keep='last'))\n",
    "\n",
    "df['keyword_len'] = df['keyword'].apply(lambda x: len(x))\n",
    "df['keyword_len'] = (df['keyword_len'] - df['keyword_len'].min()) / (df['keyword_len'].max() - df['keyword_len'].min())\n",
    "\n",
    "df['avg_monthly_searches_norm'] = np.log1p(df['avg_monthly_searches'])\n",
    "df['avg_monthly_searches_norm'] = (df['avg_monthly_searches_norm'] - df['avg_monthly_searches_norm'].min()) / (df['avg_monthly_searches_norm'].max() - df['avg_monthly_searches_norm'].min())\n",
    "\n",
    "df['score'] = df['avg_monthly_searches_norm'] - df['keyword_len']\n",
    "\n",
    "df = df.sort_values(by='score', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from time import sleep\n",
    "from urllib.parse import quote, urlparse, parse_qs\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "def setup_driver(url=\"https://trends.google.com/trends/explore\"):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.set_capability(\"goog:loggingPrefs\", {\"performance\": \"INFO\"})\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.execute_cdp_cmd(\"Network.enable\", {})\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def parse_json_body(entry):\n",
    "    body = entry.get(\"body\", \"{}\")\n",
    "    if body.startswith(\")]}'\"):\n",
    "        body = body.split(\"\\n\", 1)[1]\n",
    "    return json.loads(body)\n",
    "\n",
    "\n",
    "def get_multiline_data(driver, log_json):\n",
    "    request_id = log_json[\"params\"][\"requestId\"]\n",
    "    response_body = driver.execute_cdp_cmd(\"Network.getResponseBody\", {\"requestId\": request_id})\n",
    "    parsed = parse_json_body(response_body)\n",
    "\n",
    "    timeline = parsed[\"default\"][\"timelineData\"]\n",
    "    dates = np.array([\n",
    "        datetime.fromtimestamp(int(t[\"time\"]), tz=timezone.utc).date()\n",
    "        for t in timeline\n",
    "    ])\n",
    "\n",
    "    values_list = [t.get(\"value\", []) for t in timeline]\n",
    "    values = np.array(values_list, dtype=float)\n",
    "    if values.ndim == 1:\n",
    "        values = values.reshape(-1, 1) # Ensure values is 2D\n",
    "\n",
    "    maximums = np.max(values, axis=0)\n",
    "\n",
    "    return maximums, dates, values\n",
    "\n",
    "\n",
    "def get_related_keywords(driver, log_json):\n",
    "    full_url = log_json[\"params\"][\"response\"][\"url\"]\n",
    "    query_params = parse_qs(urlparse(full_url).query)\n",
    "    query_req = json.loads(query_params['req'][0])\n",
    "    keyword = query_req['restriction']['complexKeywordsRestriction']['keyword'][0]['value']\n",
    "\n",
    "    request_id = log_json[\"params\"][\"requestId\"]\n",
    "    response_body = driver.execute_cdp_cmd(\"Network.getResponseBody\", {\"requestId\": request_id})\n",
    "    parsed = parse_json_body(response_body)\n",
    "    related_keywords = parsed[\"default\"][\"rankedList\"]\n",
    "    \n",
    "    query_value_dict = {}\n",
    "    for keyword_dict in related_keywords[0]['rankedKeyword']:\n",
    "        query_value_dict[keyword_dict['query']] = keyword_dict['value']\n",
    "\n",
    "    return keyword, query_value_dict\n",
    "\n",
    "\n",
    "def build_batches(driver, sorted_keywords, wait, refetch_rank=2, max_retries=5):\n",
    "    batches = []\n",
    "    i = 0\n",
    "    prev_shared = None\n",
    "    retry_count = 0\n",
    "\n",
    "    while i < len(sorted_keywords):\n",
    "        if prev_shared:\n",
    "            batch_keywords = [prev_shared] + sorted_keywords[i:i+4]\n",
    "            i += 4\n",
    "        else:\n",
    "            batch_keywords = sorted_keywords[i:i+5]\n",
    "            i += 5\n",
    "        if not batch_keywords: \n",
    "            break\n",
    "\n",
    "        # clear stray logs\n",
    "        logs = driver.get_log(\"performance\") \n",
    "        while logs:\n",
    "            logs = driver.get_log(\"performance\")\n",
    "\n",
    "        encoded = [quote(kw) for kw in batch_keywords]\n",
    "        url = f\"https://trends.google.com/trends/explore?q={','.join(encoded)}&hl=en\"\n",
    "        driver.get(url)\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"TIMESERIES\")))\n",
    "        sleep(1)\n",
    "\n",
    "        current_batch_data = {\n",
    "            \"keywords\": batch_keywords,\n",
    "            \"related_keywords\": {}\n",
    "        }\n",
    "\n",
    "        logs = driver.get_log(\"performance\")\n",
    "        all_logs = logs\n",
    "        while logs:\n",
    "            sleep(1)\n",
    "            logs = driver.get_log(\"performance\")\n",
    "            all_logs.extend(logs)\n",
    "\n",
    "        try:\n",
    "            for log in all_logs:\n",
    "                try:\n",
    "                    log_json = json.loads(log[\"message\"])[\"message\"]\n",
    "                    if log_json.get(\"method\") == \"Network.responseReceived\":\n",
    "                        response_url = log_json[\"params\"][\"response\"][\"url\"]\n",
    "                        if \"api/widgetdata/multiline\" in response_url:\n",
    "                            maximums, dates, values = get_multiline_data(driver, log_json)\n",
    "                            current_batch_data['dates'] = dates\n",
    "                            current_batch_data['values'] = values\n",
    "                        elif \"api/widgetdata/relatedsearches\" in response_url:\n",
    "                            keyword, related = get_related_keywords(driver, log_json)\n",
    "                            current_batch_data['related_keywords'][keyword] = related\n",
    "                except (json.JSONDecodeError, KeyError):\n",
    "                    continue\n",
    "        except Exception:\n",
    "            print(f\"Failed to use request_id. Retrying\")\n",
    "            retry_count += 1\n",
    "            if retry_count > max_retries:\n",
    "                raise RuntimeError(f\"Exceeded max retries ({max_retries}) for batch {batch_keywords}\")\n",
    "            if prev_shared:\n",
    "                i -= 4\n",
    "            else:\n",
    "                i = 0\n",
    "            sleep(1)\n",
    "            continue\n",
    "\n",
    "        if 'values' in current_batch_data:\n",
    "            batches.append(current_batch_data)\n",
    "            retry_count = 0  # reset retry counter on success\n",
    "        else:\n",
    "            print(f\"Failed to retrieve multiline data. Retrying\")\n",
    "            retry_count += 1\n",
    "            if retry_count > max_retries:\n",
    "                raise RuntimeError(f\"Exceeded max retries ({max_retries}) for batch {batch_keywords}\")\n",
    "            if prev_shared:\n",
    "                i -= 4\n",
    "            else:\n",
    "                i = 0\n",
    "            sleep(1)\n",
    "            continue\n",
    "\n",
    "        # Pick refetch_rank keyword max for the next iteration\n",
    "        keyword_means = sorted(\n",
    "            zip(maximums, batch_keywords), \n",
    "            key=lambda x: x[0], \n",
    "            reverse=True\n",
    "        )\n",
    "        if len(keyword_means) >= refetch_rank:\n",
    "            prev_shared = keyword_means[refetch_rank - 1][1]\n",
    "        else:\n",
    "            prev_shared = keyword_means[-1][1] if keyword_means else None\n",
    "        if not prev_shared:\n",
    "            break\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def stitch_batches(batches):\n",
    "    if not batches:\n",
    "        return [], np.array([])\n",
    "\n",
    "    stitched_keywords = list(batches[0][\"keywords\"])\n",
    "    stitched_values = np.array(batches[0][\"values\"])\n",
    "    if stitched_values.ndim == 1:\n",
    "        stitched_values = stitched_values[:, np.newaxis]\n",
    "\n",
    "    for i in range(1, len(batches)):\n",
    "        prev = batches[i - 1]\n",
    "        current = batches[i]\n",
    "\n",
    "        current_values = np.array(current[\"values\"])\n",
    "        if current_values.ndim == 1:\n",
    "            current_values = current_values[:, np.newaxis]\n",
    "\n",
    "        # find shared keyword\n",
    "        shared = list(set(prev[\"keywords\"]) & set(current[\"keywords\"]))[0]\n",
    "\n",
    "        prev_idx = stitched_keywords.index(shared)\n",
    "        current_idx = current[\"keywords\"].index(shared)\n",
    "\n",
    "        mean_prev = stitched_values[:, prev_idx].mean()\n",
    "        mean_current = current_values[:, current_idx].mean()\n",
    "        scale = 0.0 if mean_current == 0 else mean_prev / mean_current\n",
    "\n",
    "        scaled = current_values * scale\n",
    "        keep_cols = [j for j, kw in enumerate(current[\"keywords\"]) if kw not in stitched_keywords]\n",
    "\n",
    "        stitched_values = np.column_stack((stitched_values, scaled[:, keep_cols]))\n",
    "        stitched_keywords.extend([current[\"keywords\"][j] for j in keep_cols])\n",
    "\n",
    "    return stitched_keywords, stitched_values\n",
    "\n",
    "\n",
    "def get_trends_data(driver, wait, sorted_keywords, refetch_rank=2):\n",
    "    if not sorted_keywords:\n",
    "        return [], np.array([]), {}\n",
    "\n",
    "    driver.get_log(\"performance\")  # clear stray logs\n",
    "    batches = build_batches(driver, sorted_keywords, wait, refetch_rank=refetch_rank)\n",
    "\n",
    "    stitched_keywords, stitched_values = stitch_batches(batches)\n",
    "\n",
    "    related_keywords = {}\n",
    "    for batch in batches:\n",
    "        for main_keyword, related_data_dict in batch.get(\"related_keywords\", {}).items():\n",
    "            if main_keyword not in related_keywords:\n",
    "                related_keywords[main_keyword] = related_data_dict\n",
    "\n",
    "    trends_df = pd.DataFrame(stitched_values, columns=stitched_keywords)\n",
    "    return trends_df, related_keywords, batches\n",
    "\n",
    "\n",
    "def resort_df(df, trends_df):\n",
    "    temp = pd.DataFrame(trends_df.mean()).reset_index().rename(columns={'index':'keyword', 0:'scrape_vol'})\n",
    "    df = df.drop(columns='scrape_vol', errors='ignore')\n",
    "    df = df.merge(temp, on='keyword')\n",
    "    return df.sort_values(by=['scrape_vol', 'score'], ascending=[False, False])\n",
    "\n",
    "\n",
    "def create_unified_keyword_series_refined(trends_df, related_keywords):\n",
    "    sorted_scalars = trends_df.mean(axis=0)\n",
    "\n",
    "    related_data = []\n",
    "    for parent_kw, rel_dict in related_keywords.items():\n",
    "        if parent_kw not in sorted_scalars:\n",
    "            continue\n",
    "        parent_mean = sorted_scalars[parent_kw]\n",
    "        for rel_kw, score in rel_dict.items():\n",
    "            scaled_val = (score / 100) * parent_mean\n",
    "            related_data.append((rel_kw, scaled_val))\n",
    "\n",
    "    if related_data:\n",
    "        related_df = pd.DataFrame(related_data, columns=['keyword', 'score'])\n",
    "        related_scalars = related_df.groupby('keyword')['score'].mean()\n",
    "    else:\n",
    "        related_scalars = pd.Series(dtype=float)\n",
    "\n",
    "    all_scalars = (\n",
    "        pd.concat([sorted_scalars, related_scalars])\n",
    "        .groupby(level=0).mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    return all_scalars\n",
    "\n",
    "\n",
    "driver = setup_driver()\n",
    "wait = WebDriverWait(driver, 20)\n",
    "input(\"Press Enter after login/CAPTCHA if needed...\")\n",
    "\n",
    "print('Fetch 1...')\n",
    "sorted_keywords = df.keyword.to_list()\n",
    "trends_df, related_keywords, batches = get_trends_data(driver, wait, sorted_keywords, refetch_rank=2)\n",
    "for i in range(2):\n",
    "    print(f'Fetch {i+2}...')\n",
    "    df = resort_df(df, trends_df)\n",
    "    sorted_keywords = df.keyword.to_list()\n",
    "    trends_df, related_keywords, batches = get_trends_data(driver, wait, sorted_keywords, refetch_rank=3)\n",
    "driver.quit()\n",
    "\n",
    "all_keywords = create_unified_keyword_series_refined(trends_df, related_keywords)\n",
    "all_keywords = all_keywords[all_keywords.index.isin([kw for kw in all_keywords.index if kw not in STOP_WORDS])]\n",
    "all_keywords = all_keywords.reset_index()\n",
    "all_keywords.columns = ['keyword', 'estimated_relative_volume']\n",
    "\n",
    "if input('save? (y/n)').lower().strip() == 'y':\n",
    "    df = resort_df(df, trends_df)\n",
    "    df['date'] = df['date'].dt.date\n",
    "    df.to_csv('data/planner_keywords_sorted.csv', index=False)\n",
    "    all_keywords.to_csv('data/all_keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def create_unified_keyword_series_refined(trends_df, related_keywords):\n",
    "    sorted_scalars = trends_df.mean(axis=0)\n",
    "\n",
    "    related_data = []\n",
    "    for parent_kw, rel_dict in related_keywords.items():\n",
    "        if parent_kw not in sorted_scalars:\n",
    "            continue\n",
    "        parent_mean = sorted_scalars[parent_kw]\n",
    "        for rel_kw, score in rel_dict.items():\n",
    "            scaled_val = (score / 100) * parent_mean\n",
    "            related_data.append((rel_kw, scaled_val))\n",
    "\n",
    "    if related_data:\n",
    "        related_df = pd.DataFrame(related_data, columns=['keyword', 'score'])\n",
    "        related_scalars = related_df.groupby('keyword')['score'].mean()\n",
    "    else:\n",
    "        related_scalars = pd.Series(dtype=float)\n",
    "\n",
    "    all_scalars = (\n",
    "        pd.concat([sorted_scalars, related_scalars])\n",
    "        .groupby(level=0).mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    return all_scalars\n",
    "\n",
    "all_keywords = create_unified_keyword_series_refined(trends_df, related_keywords)\n",
    "all_keywords = all_keywords[all_keywords.index.isin([kw for kw in all_keywords.index if kw not in STOP_WORDS])]\n",
    "all_keywords = all_keywords.reset_index()\n",
    "all_keywords.columns = ['keyword', 'estimated_relative_volume']\n",
    "all_keywords.to_csv('data/all_keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "all_keywords[all_keywords.index.str.split().str.len() == 1].index\n",
    "\n",
    "# model = GLiNER.from_pretrained(\"urchade/gliner_large-v2.1\")\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_multi-v2.1\")\n",
    "\n",
    "text = \"\"\"\n",
    "Cristiano Ronaldo dos Santos Aveiro (Portuguese pronunciation: [kɾiʃˈtjɐnu ʁɔˈnaldu]; born 5 February 1985) is a Portuguese professional footballer who plays as a forward for and captains both Saudi Pro League club Al Nassr and the Portugal national team. Widely regarded as one of the greatest players of all time, Ronaldo has won five Ballon d'Or awards,[note 3] a record three UEFA Men's Player of the Year Awards, and four European Golden Shoes, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship and the UEFA Nations League. Ronaldo holds the records for most appearances (183), goals (140) and assists (42) in the Champions League, goals in the European Championship (14), international goals (128) and international appearances (205). He is one of the few players to have made over 1,200 professional career appearances, the most by an outfield player, and has scored over 850 official senior career goals for club and country, making him the top goalscorer of all time.\n",
    "\"\"\"\n",
    "\n",
    "labels = [\"company\", \"brand\", \"organization\", \"product\"]\n",
    "\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = setup_driver()\n",
    "wait = WebDriverWait(driver, 20)\n",
    "input(\"Press Enter after login/CAPTCHA if needed...\")\n",
    "\n",
    "all_scalars_filtered = all_scalars[all_scalars.index.str.split().str.len() > 1]\n",
    "sorted_keywords = all_keywords.index.to_list()\n",
    "trends_df, related_keywords, batches = get_trends_data(driver, wait, sorted_keywords, refetch_rank=2)\n",
    "\n",
    "for _ in range(2):\n",
    "    sorted_keywords = pd.DataFrame(trends_df.mean()).reset_index().rename(columns={'index':'keyword', 0:'scrape_vol'}).keyword.to_list()\n",
    "    trends_df, related_keywords, batches = get_trends_data(driver, wait, sorted_keywords, refetch_rank=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords[all_keywords.index.str.split().str.len() == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keyword_scalars(trends_df, related_keywords):\n",
    "    sorted_scalars = trends_df.mean(axis=0)\n",
    "    \n",
    "    related_scalars = []\n",
    "    for parent_kw, rel_dict in related_keywords.items():\n",
    "        if parent_kw not in sorted_scalars:\n",
    "            continue\n",
    "        parent_mean = sorted_scalars[parent_kw]\n",
    "        for rel_kw, score in rel_dict.items():\n",
    "            scaled_val = (score / 100) * parent_mean\n",
    "            related_scalars.append((rel_kw, scaled_val))\n",
    "    \n",
    "    related_scalars = pd.Series(dict(related_scalars))\n",
    "    \n",
    "    all_scalars = (\n",
    "        pd.concat([sorted_scalars, related_scalars])\n",
    "        .groupby(level=0).mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    return all_scalars\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
