{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('data/*.html')\n",
    "list_of_dfs = []\n",
    "for file_path in file_paths:\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    header = soup.find('div', class_='particle-table-header')\n",
    "\n",
    "    if not header:\n",
    "        print(f\"  --> Warning: Could not find a header in {file_path}. Skipping this file.\")\n",
    "        continue\n",
    "\n",
    "    columns = [cell.text.strip() for cell in header.find_all('aw-header-cell')]\n",
    "\n",
    "    rows = []\n",
    "    for row in soup.find_all('div', class_='particle-table-row'):\n",
    "        cells = row.find_all('ess-cell')\n",
    "        if len(cells) == len(columns):\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            rows.append(row_data)\n",
    "        else:\n",
    "            print(f\"  --> Warning: Mismatch between column count and cell count in a row within {file_path}.\")\n",
    "\n",
    "    df_current_file = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    date_str_match = re.search(r'keywords_(\\d{2}-\\d{2}-\\d{2})\\.html', file_path)\n",
    "    if date_str_match:\n",
    "        date_str = date_str_match.group(1)\n",
    "        year_part = date_str.split('-')[0]\n",
    "        full_year_str = '20' + year_part\n",
    "        corrected_date_str = f\"{full_year_str}-{date_str.split('-')[1]}-{date_str.split('-')[2]}\"\n",
    "        df_current_file['date'] = pd.to_datetime(corrected_date_str)\n",
    "    else:\n",
    "        df_current_file['date'] = pd.NaT\n",
    "\n",
    "\n",
    "    list_of_dfs.append(df_current_file)\n",
    "\n",
    "if list_of_dfs:\n",
    "    df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "else:\n",
    "    print(\"No valid HTML files were found or processed. The final DataFrame is empty.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "def to_snake_case(column_name):\n",
    "    s = re.sub(r'[\\s\\n]+', ' ', column_name).strip()\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[().]', '', s)\n",
    "    s = s.replace(' ', '_')\n",
    "    s = s.replace('top_of_page_', '')\n",
    "    return s\n",
    "\n",
    "original_columns = df.columns\n",
    "new_columns_map = {col: to_snake_case(col) for col in original_columns}\n",
    "df.rename(columns=new_columns_map, inplace=True)\n",
    "\n",
    "df = df.drop(columns=['account_status', 'ad_impression_share'])\n",
    "\n",
    "df['competition'] = df['competition'].map({'Low': 1, 'Medium': 2, 'High': 3})\n",
    "df['keyword'] = df['keyword'].apply(lambda x: re.sub(r'[\\s\\n]+', ' ', x).strip())\n",
    "\n",
    "df['avg_monthly_searches'] = df['avg_monthly_searches'].str.split(' –').str[0]\n",
    "df['avg_monthly_searches'] = df['avg_monthly_searches'].str.replace('K', '000', regex=False)\n",
    "df['avg_monthly_searches'] = df['avg_monthly_searches'].str.replace('M', '000000', regex=False)\n",
    "df['avg_monthly_searches'] = pd.to_numeric(df['avg_monthly_searches'], errors='coerce')\n",
    "\n",
    "for col in ['bid_low_range', 'bid_high_range']:\n",
    "    df[col] = df[col].str.replace('€', '', regex=False)\n",
    "    df[col] = df[col].str.replace('—', '', regex=False).str.strip()\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df['avg_bid'] = df[['bid_high_range', 'bid_low_range']].mean(axis=1)\n",
    "\n",
    "for col in ['three_month_change', 'yoy_change']:\n",
    "    df[col] = df[col].str.replace('%', '', regex=False)\n",
    "    df[col] = df[col].str.replace(',', '', regex=False)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df[col] /= 100\n",
    "\n",
    "df = (df.sort_values(by='date')\n",
    "        .drop_duplicates(subset='keyword', keep='last'))\n",
    "\n",
    "df['keyword_len'] = df['keyword'].apply(lambda x: len(x))\n",
    "df['keyword_len'] = (df['keyword_len'] - df['keyword_len'].min()) / (df['keyword_len'].max() - df['keyword_len'].min())\n",
    "\n",
    "df['avg_monthly_searches_norm'] = np.log1p(df['avg_monthly_searches'])\n",
    "df['avg_monthly_searches_norm'] = (df['avg_monthly_searches_norm'] - df['avg_monthly_searches_norm'].min()) / (df['avg_monthly_searches_norm'].max() - df['avg_monthly_searches_norm'].min())\n",
    "\n",
    "df['score'] = df['avg_monthly_searches_norm'] - df['keyword_len']\n",
    "df['relative_volume'] = np.nan\n",
    "\n",
    "df = df.sort_values(by='score', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from time import sleep\n",
    "from urllib.parse import quote, urlparse, parse_qs\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fundamental set up\n",
    "def setup_driver(url=\"https://trends.google.com/trends/explore\"):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.set_capability(\"goog:loggingPrefs\", {\"performance\": \"INFO\"})\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.execute_cdp_cmd(\"Network.enable\", {})\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def parse_json_body(entry):\n",
    "    body = entry.get(\"body\", \"{}\")\n",
    "    if body.startswith(\")]}'\"):\n",
    "        body = body.split(\"\\n\", 1)[1]\n",
    "    return json.loads(body)\n",
    "\n",
    "\n",
    "def get_multiline_data(driver, log_json):\n",
    "    request_id = log_json[\"params\"][\"requestId\"]\n",
    "    response_body = driver.execute_cdp_cmd(\"Network.getResponseBody\", {\"requestId\": request_id})\n",
    "    parsed = parse_json_body(response_body)\n",
    "\n",
    "    timeline = parsed[\"default\"][\"timelineData\"]\n",
    "    dates = np.array([\n",
    "        datetime.fromtimestamp(int(t[\"time\"]), tz=timezone.utc).date()\n",
    "        for t in timeline\n",
    "    ])\n",
    "\n",
    "    values_list = [t.get(\"value\", []) for t in timeline]\n",
    "    values = np.array(values_list, dtype=float)\n",
    "    if values.ndim == 1:\n",
    "        values = values.reshape(-1, 1) # Ensure values is 2D\n",
    "\n",
    "    maximums = np.max(values, axis=0)\n",
    "\n",
    "    return maximums, dates, values\n",
    "\n",
    "\n",
    "def get_related_keywords(driver, log_json):\n",
    "    full_url = log_json[\"params\"][\"response\"][\"url\"]\n",
    "    query_params = parse_qs(urlparse(full_url).query)\n",
    "    query_req = json.loads(query_params['req'][0])\n",
    "    keyword = query_req['restriction']['complexKeywordsRestriction']['keyword'][0]['value']\n",
    "\n",
    "    request_id = log_json[\"params\"][\"requestId\"]\n",
    "    response_body = driver.execute_cdp_cmd(\"Network.getResponseBody\", {\"requestId\": request_id})\n",
    "    parsed = parse_json_body(response_body)\n",
    "    related_keywords = parsed[\"default\"][\"rankedList\"]\n",
    "    \n",
    "    query_value_dict = {}\n",
    "    for keyword_dict in related_keywords[0]['rankedKeyword']:\n",
    "        query_value_dict[keyword_dict['query']] = keyword_dict['value']\n",
    "\n",
    "    return keyword, query_value_dict\n",
    "\n",
    "\n",
    "def get_url(driver, batch_keywords):\n",
    "    while driver.get_log(\"performance\"): # clear stray logs\n",
    "        pass\n",
    "    encoded = [quote(kw) for kw in batch_keywords]\n",
    "    url = f\"https://trends.google.com/trends/explore?q={','.join(encoded)}&hl=en\"\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 120).until(EC.presence_of_element_located((By.ID, \"TIMESERIES\")))\n",
    "\n",
    "\n",
    "def parse_logs(driver, batch_keywords):\n",
    "    current_batch_data = {\n",
    "        \"keywords\": batch_keywords,\n",
    "        \"related_keywords\": {kw:{} for kw in batch_keywords}\n",
    "    }\n",
    "\n",
    "    logs = driver.get_log(\"performance\")\n",
    "    all_logs = logs\n",
    "    while logs:\n",
    "        sleep(1)\n",
    "        logs = driver.get_log(\"performance\")\n",
    "        all_logs.extend(logs)\n",
    "\n",
    "    for log in all_logs:\n",
    "        log_json = json.loads(log[\"message\"])[\"message\"]\n",
    "        if log_json.get(\"method\") == \"Network.responseReceived\":\n",
    "            response_url = log_json[\"params\"][\"response\"][\"url\"]\n",
    "            if \"api/widgetdata/multiline\" in response_url:\n",
    "                maximums, dates, values = get_multiline_data(driver, log_json)\n",
    "                current_batch_data['dates'] = dates\n",
    "                current_batch_data['values'] = values\n",
    "                current_batch_data['maximums'] = maximums\n",
    "            elif \"api/widgetdata/relatedsearches\" in response_url:\n",
    "                keyword, related = get_related_keywords(driver, log_json)\n",
    "                current_batch_data['related_keywords'][keyword] = related\n",
    "    \n",
    "    return current_batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest\n",
    "def subset_batch(batch, keywords):\n",
    "    indices = [batch[\"keywords\"].index(kw) for kw in keywords if kw in batch[\"keywords\"]]\n",
    "    values = batch[\"values\"][:, indices]\n",
    "    maximums = [batch[\"maximums\"][i] for i in indices]\n",
    "    related_keywords = {kw: batch[\"related_keywords\"].get(kw, {}) for kw in keywords}\n",
    "    \n",
    "    return {\n",
    "        \"dates\": batch[\"dates\"],\n",
    "        \"keywords\": keywords,\n",
    "        \"values\": values,\n",
    "        \"maximums\": maximums,\n",
    "        \"related_keywords\": related_keywords,\n",
    "    }\n",
    "\n",
    "\n",
    "def detect_outlier(values, keywords, bridge_rank=3, min_mean_volume=2):\n",
    "    means = np.mean(values, axis=0)\n",
    "    nonzero = means[means > min_mean_volume]\n",
    "    outlier = keywords[np.argmax(means)]\n",
    "    if len(nonzero) <= min(len(keywords), bridge_rank) and len(keywords):\n",
    "        return True, outlier\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def detect_low_outliers(values, keywords, bridge_keyword, min_mean_volume=2):\n",
    "    means = np.mean(values, axis=0)\n",
    "    low_outliers_idx = np.where(means < min_mean_volume)[0]\n",
    "    low_outliers = [keywords[i] for i in low_outliers_idx]\n",
    "    high_volumes = [kw for kw in keywords if kw not in low_outliers]\n",
    "\n",
    "    if low_outliers:\n",
    "        if bridge_keyword in low_outliers:\n",
    "            return high_volumes\n",
    "        \n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def choose_bridge_keyword(batch_keywords, values, bridge_rank=3, min_mean_volume=2):\n",
    "    means = np.mean(values, axis=0)\n",
    "    kw_means = sorted(zip(batch_keywords, means), key=lambda x: x[1], reverse=True)\n",
    "    kw_means = [kw_mean for kw_mean in kw_means if kw_mean[1] > min_mean_volume]\n",
    "    if len(kw_means) >= bridge_rank:\n",
    "        return kw_means[bridge_rank - 1][0]\n",
    "    else:\n",
    "        return kw_means[-1][0]\n",
    "\n",
    "\n",
    "def scrape_batches(driver, sorted_keywords, min_mean_volume=2, max_retries=3, bridge_rank=3):\n",
    "    batches, retry_count, bridge_keyword = [], 0, None\n",
    "    \n",
    "    keywords_to_process = list(sorted_keywords)\n",
    "    while keywords_to_process:\n",
    "        if bridge_keyword:\n",
    "            new_keywords = keywords_to_process[:4]\n",
    "            batch_keywords = [bridge_keyword] + new_keywords\n",
    "        else:\n",
    "            batch_keywords = new_keywords = keywords_to_process[:5]\n",
    "        batch_keywords = list(set(batch_keywords))\n",
    "\n",
    "        try:\n",
    "            get_url(driver, batch_keywords)\n",
    "            current_batch_dict = parse_logs(driver, batch_keywords)\n",
    "        except Exception:\n",
    "            retry_count += 1\n",
    "            if retry_count > max_retries:\n",
    "                print(f\"Exceeded max retries ({max_retries})\")\n",
    "                return batches, []\n",
    "            continue\n",
    "        if 'values' not in current_batch_dict:\n",
    "            retry_count += 1\n",
    "            if retry_count > max_retries:\n",
    "                print(f\"Exceeded max retries ({max_retries})\")\n",
    "                batches, []\n",
    "            continue\n",
    "\n",
    "        values = np.array(current_batch_dict[\"values\"])\n",
    "        retry_count = 0\n",
    "        keywords_to_process = keywords_to_process[len(new_keywords):]\n",
    "\n",
    "        outlier_kw = detect_outlier(values,\n",
    "                                    batch_keywords,\n",
    "                                    bridge_keyword,\n",
    "                                    min_mean_volume=min_mean_volume)\n",
    "        if outlier_kw:\n",
    "            if not bridge_keyword or bridge_keyword == outlier_kw:\n",
    "                bridge_keyword = choose_bridge_keyword(batch_keywords,\n",
    "                                                       values,\n",
    "                                                       bridge_rank=bridge_rank,\n",
    "                                                       min_mean_volume=min_mean_volume)\n",
    "\n",
    "            distilled_batch = subset_batch(current_batch_dict, [bridge_keyword, outlier_kw])\n",
    "            batches.append(distilled_batch)\n",
    "\n",
    "            leftover_keywords = [kw for kw in new_keywords if kw != outlier_kw]\n",
    "            keywords_to_process = leftover_keywords + keywords_to_process\n",
    "            print(f\"{bridge_keyword}\")\n",
    "            print(f\"Outlier: {outlier_kw} -- {current_batch_dict['keywords']}\")\n",
    "        else:\n",
    "            batches.append(current_batch_dict)\n",
    "            print(f\"{bridge_keyword}\")\n",
    "            print(f\"{current_batch_dict['keywords']}\")\n",
    "            bridge_keyword = choose_bridge_keyword(batch_keywords,\n",
    "                                                   values,\n",
    "                                                   bridge_rank=bridge_rank,\n",
    "                                                   min_mean_volume=min_mean_volume)\n",
    "\n",
    "    return batches, keywords_to_process\n",
    "\n",
    "\n",
    "def join_batches(batches):\n",
    "    if not batches:\n",
    "        return np.array([]), []\n",
    "\n",
    "    concatenated_keywords = list(batches[0][\"keywords\"])\n",
    "    concatenated_values = np.array(batches[0][\"values\"])\n",
    "    if concatenated_values.ndim == 1:\n",
    "        concatenated_values = concatenated_values[:, np.newaxis]\n",
    "\n",
    "    i = 1\n",
    "    while i < len(batches):\n",
    "        current = batches[i]\n",
    "        current_values = np.array(current[\"values\"])\n",
    "        if current_values.ndim == 1:\n",
    "            current_values = current_values[:, np.newaxis]\n",
    "\n",
    "        bridge_keywords_set = set(concatenated_keywords) & set(current[\"keywords\"])\n",
    "        bridge_keyword = list(bridge_keywords_set)[0]\n",
    "        prev_idx = concatenated_keywords.index(bridge_keyword)\n",
    "        current_idx = current[\"keywords\"].index(bridge_keyword)\n",
    "\n",
    "        mean_prev = concatenated_values[:, prev_idx].mean()\n",
    "        mean_current = current_values[:, current_idx].mean()\n",
    "        scale = mean_prev / np.maximum(mean_current, 1e-9)\n",
    "\n",
    "        scaled = current_values * scale\n",
    "        keep_cols = [j for j, kw in enumerate(current[\"keywords\"]) if kw not in concatenated_keywords]\n",
    "        concatenated_values = np.column_stack((concatenated_values, scaled[:, keep_cols]))\n",
    "        concatenated_keywords.extend([current[\"keywords\"][j] for j in keep_cols])\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return concatenated_values, concatenated_keywords\n",
    "\n",
    "\n",
    "def resort_df(df, trends_df):\n",
    "    new_relative_volume = pd.DataFrame(trends_df.mean()).reset_index().rename(columns={'index':'keyword', 0:'relative_volume'})\n",
    "    df = df.drop(columns='relative_volume', errors='ignore')\n",
    "    df = df.merge(new_relative_volume, on='keyword', how='left')\n",
    "    return df.sort_values(by=['relative_volume', 'score'], ascending=[False, False])\n",
    "\n",
    "\n",
    "def get_trends_data(driver, initial_df, min_mean_volume=2, bridge_rank=3):\n",
    "    keywords_to_process = initial_df['keyword'].tolist()\n",
    "    \n",
    "    batches = scrape_batches(driver,\n",
    "                             keywords_to_process,\n",
    "                             min_mean_volume=min_mean_volume,\n",
    "                             bridge_rank=bridge_rank)\n",
    "    \n",
    "    joined_values, joined_keywords = join_batches(batches)\n",
    "    trends_df = pd.DataFrame(joined_values, columns=joined_keywords)\n",
    "    final_df = resort_df(initial_df, trends_df)\n",
    "    \n",
    "    related_keywords = {}\n",
    "    for batch in batches:\n",
    "        related_keywords.update(batch.get(\"related_keywords\", {}))\n",
    "    \n",
    "    return final_df, trends_df, related_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/planner_keywords_sorted.csv')\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# driver = setup_driver()\n",
    "# input(\"Press Enter after login/CAPTCHA if needed...\")\n",
    "\n",
    "BRIDGE_RANK = 3\n",
    "MIN_MEAN_VOLUME = 3\n",
    "keywords_to_process = df['keyword'].unique().tolist()[-15:]\n",
    "\n",
    "\n",
    "stored_and_sorted = set()\n",
    "max_iterations = len(df) // 10\n",
    "\n",
    "all_batches = []\n",
    "for i in range(max_iterations):\n",
    "    print(f\"\\nIteration {i+1} - Keywords remaining: {len(keywords_to_process)}\")\n",
    "    newly_scraped_batches, remaining_keywords = scrape_batches(driver,\n",
    "                                                               keywords_to_process,\n",
    "                                                               bridge_rank=BRIDGE_RANK,\n",
    "                                                               min_mean_volume=MIN_MEAN_VOLUME)\n",
    "    all_batches.extend(newly_scraped_batches)\n",
    "\n",
    "    joined_values, joined_keywords = join_batches(all_batches)\n",
    "    trends_df = pd.DataFrame(joined_values, columns=joined_keywords)\n",
    "    df = resort_df(df, trends_df)\n",
    "\n",
    "    if remaining_keywords:\n",
    "        next_new_keyword = remaining_keywords[0]\n",
    "        resume_index = df.index[df['keyword'] == next_new_keyword][0]\n",
    "        start_index = max(0, resume_index - 1)\n",
    "        keywords_to_process = [k for k in df['keyword'].iloc[start_index:].tolist() if k not in stored_and_sorted]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "joined_values, joined_keywords = join_batches(all_batches)\n",
    "trends_df = pd.DataFrame(joined_values, columns=joined_keywords)\n",
    "df = resort_df(df, trends_df)\n",
    "df\n",
    "\n",
    "# # Final pass\n",
    "# final_batches = []\n",
    "# print(\"\\nFinal iteration\")    \n",
    "# final_batches, remaining_keywords = scrape_batches(\n",
    "#     driver, \n",
    "#     df['keyword'].to_list(), \n",
    "#     bridge_rank=BRIDGE_RANK, \n",
    "#     min_mean_volume=MIN_MEAN_VOLUME,\n",
    "# )\n",
    "# if remaining_keywords:\n",
    "#     print(\"Final pass was incomplete:\")\n",
    "#     print(remaining_keywords)\n",
    "\n",
    "# joined_values, joined_keywords = join_batches(final_batches)\n",
    "# trends_df = pd.DataFrame(joined_values, columns=joined_keywords)\n",
    "# df = resort_df(df, trends_df)\n",
    "# df['date'] = df['date'].dt.date\n",
    "# df.to_csv('data/planner_keywords_sorted.csv', index=False)\n",
    "\n",
    "# related_keywords = {}\n",
    "# for batch in all_batches:\n",
    "#     related_keywords.update(batch.get(\"related_keywords\", {}))\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unified_keyword_series_refined(trends_df, related_keywords):\n",
    "    sorted_scalars = trends_df.mean(axis=0)\n",
    "\n",
    "    related_data = []\n",
    "    for parent_kw, rel_dict in related_keywords.items():\n",
    "        if parent_kw not in sorted_scalars:\n",
    "            continue\n",
    "        parent_mean = sorted_scalars[parent_kw]\n",
    "        for rel_kw, score in rel_dict.items():\n",
    "            scaled_val = (score / 100) * parent_mean\n",
    "            related_data.append((rel_kw, scaled_val))\n",
    "\n",
    "    if related_data:\n",
    "        related_df = pd.DataFrame(related_data, columns=['keyword', 'score'])\n",
    "        related_scalars = related_df.groupby('keyword')['score'].mean()\n",
    "    else:\n",
    "        related_scalars = pd.Series(dtype=float)\n",
    "\n",
    "    all_scalars = (\n",
    "        pd.concat([sorted_scalars, related_scalars])\n",
    "        .groupby(level=0).mean()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    return all_scalars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_keywords = create_unified_keyword_series_refined(trends_df, related_keywords)\n",
    "# all_keywords = all_keywords[all_keywords.index.isin([kw for kw in all_keywords.index if kw not in STOP_WORDS])]\n",
    "# all_keywords = all_keywords[all_keywords.index.isin([kw for kw in all_keywords.index if kw not in STOP_WORDS])]\n",
    "# all_keywords = all_keywords.reset_index()\n",
    "# all_keywords.columns = ['keyword', 'estimated_relative_volume']\n",
    "# all_keywords['keyword_len'] = all_keywords['keyword'].apply(lambda x: len(x))\n",
    "# all_keywords['keyword_len'] = (all_keywords['keyword_len'] - all_keywords['keyword_len'].min()) / (all_keywords['keyword_len'].max() - all_keywords['keyword_len'].min())\n",
    "\n",
    "# all_keywords['avg_monthly_searches_norm'] = np.log1p(all_keywords['estimated_relative_volume'])\n",
    "# all_keywords['avg_monthly_searches_norm'] = (all_keywords['avg_monthly_searches_norm'] - all_keywords['avg_monthly_searches_norm'].min()) / (all_keywords['avg_monthly_searches_norm'].max() - all_keywords['avg_monthly_searches_norm'].min())\n",
    "\n",
    "# all_keywords['score'] = all_keywords['avg_monthly_searches_norm'] - all_keywords['keyword_len']\n",
    "\n",
    "# df = all_keywords.copy()\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = pd.read_csv('data/all_keywords.csv')\n",
    "df = pd.read_csv('data/planner_keywords_sorted.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "\n",
    "all_keywords = create_unified_keyword_series_refined(trends_df, related_keywords)\n",
    "all_keywords = all_keywords[all_keywords.index.isin([kw for kw in all_keywords.index if kw not in STOP_WORDS])]\n",
    "all_keywords = all_keywords.reset_index()\n",
    "all_keywords.columns = ['keyword', 'estimated_relative_volume']\n",
    "\n",
    "\n",
    "if input('save? (y/n)').lower().strip() == 'y':\n",
    "    df = resort_df(df, trends_df)\n",
    "    df['date'] = df['date'].dt.date\n",
    "    df.to_csv('data/planner_keywords_sorted.csv', index=False)\n",
    "    all_keywords.to_csv('data/all_keywords.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
